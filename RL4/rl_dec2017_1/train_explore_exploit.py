




import sys
for i in range(len(sys.path)):
    if 'er/Documents' in sys.path[i]:
        sys.path.remove(sys.path[i])#[i]
        break

from os.path import expanduser
home = expanduser("~")

import copy
import glob
import os
import time

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler


sys.path.insert(0, '../baselines/')
sys.path.insert(0, '../baselines/baselines/common/vec_env')
from subproc_vec_env import SubprocVecEnv

sys.path.insert(0, './utils/')
from envs import make_env, make_env_monitor, make_env_basic


# from agent_modular2 import a2c
# from agent_modular2 import ppo
# from agent_modular2 import a2c_minibatch
# from agent_modular2 import a2c_list_rollout
# from agent_modular2 import a2c_with_var

from a2c_agents import a2c



from train_utils import do_vid, do_gifs, do_params, do_ls, update_ls_plot, view_reward_episode, do_prob_state, do_gifs2

from train_utils2 import do_gifs3, do_ls_2, update_ls_plot_2

sys.path.insert(0, './visualizations/')
from make_plots import make_plots

import argparse
import json
import subprocess
import pickle

from vae import VAE





def train(model_dict):

    def update_current_state(current_state, state, channels):
        # current_state: [processes, channels*stack, height, width]
        state = torch.from_numpy(state).float()  # (processes, channels, height, width)
        # if num_stack > 1:
        #first stack*channel-channel frames = last stack*channel-channel , so slide them forward
        current_state[:, :-channels] = current_state[:, channels:] 
        current_state[:, -channels:] = state #last frame is now the new one

        return current_state


    def update_rewards(reward, done, final_rewards, episode_rewards, current_state):
        # Reward, Done: [P], [P]
        # final_rewards, episode_rewards: [P,1]. [P,1]
        # current_state: [P,C*S,H,W]
        reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float() #[P,1]
        episode_rewards += reward #keeps track of current episode cumulative reward
        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done]) #[P,1]
        final_rewards *= masks #erase the ones that are done
        final_rewards += (1 - masks) * episode_rewards  #set it to the cumulative episode reward
        episode_rewards *= masks #erase the done ones
        masks = masks.type(dtype) #cuda
        if current_state.dim() == 4:  # if state is a frame/image
            current_state *= masks.unsqueeze(2).unsqueeze(2)  #[P,1,1,1]
        else:
            current_state *= masks   #restart the done ones, by setting the state to zero
        return reward, masks, final_rewards, episode_rewards, current_state



    num_frames = model_dict['num_frames']
    cuda = model_dict['cuda']
    which_gpu = model_dict['which_gpu']
    num_steps = model_dict['num_steps']
    num_processes = model_dict['num_processes']
    seed = model_dict['seed']
    env_name = model_dict['env']
    save_dir = model_dict['save_to']
    num_stack = model_dict['num_stack']
    algo = model_dict['algo']
    save_interval = model_dict['save_interval']
    log_interval = model_dict['log_interval']

    save_params = model_dict['save_params']
    vid_ = model_dict['vid_']
    gif_ = model_dict['gif_']
    ls_ = model_dict['ls_']
    vae_ = model_dict['vae_']
    explore_ = model_dict['explore_']

    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['CUDA_VISIBLE_DEVICES'] = str(which_gpu)

    if cuda:
        torch.cuda.manual_seed(seed)
        dtype = torch.cuda.FloatTensor
        model_dict['dtype']=dtype
    else:
        torch.manual_seed(seed)
        dtype = torch.FloatTensor
        model_dict['dtype']=dtype


    # Create environments
    print (num_processes, 'processes')
    monitor_rewards_dir = os.path.join(save_dir, 'monitor_rewards')
    if not os.path.exists(monitor_rewards_dir):
        os.makedirs(monitor_rewards_dir)
        print ('Made dir', monitor_rewards_dir) 
    envs = SubprocVecEnv([make_env(env_name, seed, i, monitor_rewards_dir) for i in range(num_processes)])


    if vid_:
        print ('env for video')
        envs_video = make_env_monitor(env_name, save_dir)

    if gif_:
        print ('env for gif')
        envs_gif = make_env_basic(env_name)

    if ls_:
        print ('env for ls')
        envs_ls = make_env_basic(env_name)

    if vae_:
        print ('env for vae')
        envs_vae = make_env_basic(env_name)


    obs_shape = envs.observation_space.shape  # (channels, height, width)
    obs_shape = (obs_shape[0] * num_stack, *obs_shape[1:])  # (channels*stack, height, width)
    shape_dim0 = envs.observation_space.shape[0]  #channels

    model_dict['obs_shape']=obs_shape
    model_dict['shape_dim0']=shape_dim0

    next_state_pred_ = 0
    model_dict['next_state_pred_'] = next_state_pred_
    


    # Create agent
    # if algo == 'a2c':

        # agent = a2c(envs, model_dict)
        
    # elif algo == 'ppo':
    #     agent = ppo(envs, model_dict)
    #     print ('init ppo agent')
    # elif algo == 'a2c_minibatch':
    #     agent = a2c_minibatch(envs, model_dict)
    #     print ('init a2c_minibatch agent')
    # elif algo == 'a2c_list_rollout':
    #     agent = a2c_list_rollout(envs, model_dict)
    #     print ('init a2c_list_rollout agent')
    # elif algo == 'a2c_with_var':
    #     agent = a2c_with_var(envs, model_dict)
    #     print ('init a2c_with_var agent')
    # elif algo == 'a2c_bin_mask':
    #     agent = a2c_with_var(envs, model_dict)
    #     print ('init a2c_with_var agent')
    # agent = model_dict['agent'](envs, model_dict)

    # #Load model
    # if args.load_path != '':
    #     # agent.actor_critic = torch.load(os.path.join(args.load_path))
    #     agent.actor_critic = torch.load(args.load_path).cuda()
    #     print ('loaded ', args.load_path)




    # see_reward_episode = 0
    # if 'Montez' in env_name and see_reward_episode:
    #     states_list = [[] for i in range(num_processes)]

    # view_reward_episode(model_dict=model_dict, frames=[])
    # dfasddsf


    # if vae_:
    #     vae = VAE()
    #     vae.cuda()



    print ('init exploit a2c agent')
    agent_exploit = a2c(envs, model_dict)

    if explore_:
        print ('init explore a2c agent')
        agent_explore = a2c(envs, model_dict)
        print ('init vae')
        vae = VAE()
        vae.cuda()

    # Init state
    state = envs.reset()  # (processes, channels, height, width)
    current_state = torch.zeros(num_processes, *obs_shape)  # (processes, channels*stack, height, width)
    current_state = update_current_state(current_state, state, shape_dim0).type(dtype) #add the new frame, remove oldest
    
    agent_exploit.insert_first_state(current_state) #storage has states: (num_steps + 1, num_processes, *obs_shape), set first step 
    if explore_:
        agent_explore.insert_first_state(current_state) #storage has states: (num_steps + 1, num_processes, *obs_shape), set first step 

    # These are used to compute average rewards for all processes.
    episode_rewards = torch.zeros([num_processes, 1]) #keeps track of current episode cumulative reward
    final_rewards = torch.zeros([num_processes, 1])

    num_updates = int(num_frames) // num_steps // num_processes
    save_interval_num_updates = int(save_interval /num_processes/num_steps)

    # prev_action = Variable(torch.zeros([num_processes, 1]).type(torch.LongTensor)).cuda()


    # For normalizing the logprobs
    B = .99
    m = torch.FloatTensor([-100.]).cuda()
    v = torch.FloatTensor([10000.]).cuda()

    # prev_reward = torch.ones(num_processes,1).cuda()
    if model_dict['init_exploit_processes'] == -1:
        init_exploit_processes = num_processes
    else:
        init_exploit_processes = model_dict['init_exploit_processes']
    exploit_processes = init_exploit_processes
    # explore_processes = 16

    all_frames = []

    start = time.time()
    start2 = time.time()
    for j in range(num_updates):

        start3 = time.time()
        for step in range(num_steps):

            # start3 = time.time()
            state_pytorch = Variable(agent_exploit.rollouts.states[step])#, volatile=True) # [P,S,84,84]

            # exploit_state = state_pytorch[:exploit_processes]
            # explore_state = state_pytorch[exploit_processes:]

            u_value, u_action, u_action_log_probs, u_dist_entropy = agent_exploit.act(state_pytorch)
            if explore_:
                r_value, r_action, r_action_log_probs, r_dist_entropy = agent_explore.act(state_pytorch)

            u_cpu_actions = u_action.data.squeeze(1).cpu().numpy() #[P]
            if explore_:
                r_cpu_actions = r_action.data.squeeze(1).cpu().numpy() #[P]

            #Choose how many you want from each
            cpu_actions = np.concatenate((u_cpu_actions[:exploit_processes],r_cpu_actions[exploit_processes:]),0) #[P]
            # cpu_actions = u_cpu_actions

            # before_step_time = time.time() - start3

            # Step, S:[P,C,H,W], R:[P], D:[P]
            # start3 = time.time()
            state, reward, done, info = envs.step(cpu_actions) 
            # step_time = time.time() - start3
            # reward_numpy = reward
            # print (reward)

            for p in range(len(state)):
                # print (state[p].shape) #[1,84,84]
                # fasad
                all_frames.append(state[p])
                print (len(all_frames))
                if len(all_frames) == 10000:
                    pickle.dump( all_frames, open(home + '/Documents/tmp/montezum_frames.pkl' , "wb" ) )
                    print ('saved pkl')
                    fafaadsfs



            # start3 = time.time()
            # Record rewards and update state
            reward, masks, final_rewards, episode_rewards, current_state = update_rewards(reward, done, final_rewards, episode_rewards, current_state)
            current_state = update_current_state(current_state, state, shape_dim0)
            # current_state_u = current_state[:exploit_processes]
            # current_state_r = current_state[exploit_processes:]

            #Insert data for exploit agent
            agent_exploit.insert_data(step, current_state, u_action.data, u_value, reward, masks, u_action_log_probs, u_dist_entropy, 0) #, done)

            if explore_:
                # Insert log prob for explore agent
                batch = state_pytorch[:,-1] #last of stack
                batch = batch.contiguous() # [P,84,84]
                elbo = vae.forward2(batch, k=10) #[P]
                elbo = elbo.view(-1,1).data  #[P,1]
                elbo = (elbo - m) / torch.sqrt(v)
                elbo = torch.clamp(elbo, max=.01) 
                agent_explore.insert_data(step, current_state, r_action.data, r_value, -elbo, masks, r_action_log_probs, r_dist_entropy, 0) #, done)

                
                #update m and v
                m = B*m + (1.-B)*elbo.mean()
                v = B*v + (1.-B)*elbo.pow(2).mean()

                if elbo.mean() < -9000.:
                    print (elbo)
                    print (reward)
                    print (elbo.mean())
                    print (elbo.pow(2).mean())
                    fadsads

            # after_step_time = time.time() - start3




            # if 'Montez' in env_name and see_reward_episode:

            #     for state_i in range(len(state)):
            #         if done[state_i]:
            #             states_list[state_i] = []
            #         else:
            #             states_list[state_i].append(np.squeeze(state[state_i]))

            #             # print (state[state_i].shape)
            #             # fasdf

            #         # print (reward)

            #         if reward_numpy[state_i] >0:
            #             #plot the states of state_i
            #             print (len(states_list[state_i]))
            #             # view_reward_episode(model_dict=model_dict, frames=states_list[state_i][len(states_list[state_i])-100:])
            #             # view_reward_episode(model_dict=model_dict, frames=states_list[state_i][len(states_list[state_i])-100:])
            #             view_reward_episode(model_dict=model_dict, frames=states_list[state_i])

            #             fadsa

            #      # and np.sum(agent.rollouts.rewards.cpu().numpy()) > 0

            #     # print (np.sum(agent.rollouts.rewards.cpu().numpy()))
            #     # print (j)

                




        steps_time = time.time() - start3
        start3 = time.time()

        #Optimize agents
        agent_exploit.update()  #agent.update(j,num_updates)
        if explore_:
            agent_explore.update()  #agent.update(j,num_updates)

            #Optimize vae
            batch = agent_exploit.rollouts.states
            batch = batch[1:]  # [Steps,Processes,Stack,84,84]
            batch = batch[:,:,0] # [Steps,Processes,84,84]
            batch = batch.contiguous().view(-1,84,84)   # [Steps*Processes,84,84]
            elbo = vae.update(batch)

        #Insert state
        agent_exploit.insert_first_state(agent_exploit.rollouts.states[-1])
        if explore_:
            agent_explore.insert_first_state(agent_explore.rollouts.states[-1])



        total_num_steps = (j + 1) * num_processes * num_steps



        #Change number of explore vs exploit
        if model_dict['init_exploit_processes'] != -1 and model_dict['inc_exploiters_over'] != -1:
            frac_step = np.minimum( (total_num_steps+1.) / float(model_dict['inc_exploiters_over']), 1.)  #fraction of steps
            aaa = int((num_processes - init_exploit_processes) * frac_step)
            exploit_processes = np.minimum(init_exploit_processes + aaa + 1, num_processes)

        update_time = time.time() - start3

        # agent_exploit.rollouts.reset_lists()
        # agent_explore.rollouts.reset_lists()



        # print ('init ', init_exploit_processes)
        # print ('cur ', exploit_processes)
        # print ('frac_step', frac_step)
        # print ('aaa', aaa)












        

        # print (agent.state_pred_error.data.cpu().numpy())


        # print ('save_interval_num_updates', save_interval_num_updates)
        # print ('num_updates', num_updates)
        # print ('j', j)

        
        # if total_num_steps % save_interval == 0 and save_dir != "":
        if j % save_interval_num_updates == 0 and save_dir != "" and j != 0:

            #Save model
            if save_params:
                do_params(save_dir, agent, total_num_steps, model_dict)
            #make video
            if vid_:
                do_vid(envs_video, update_current_state, shape_dim0, dtype, agent, model_dict, total_num_steps)
            #make gif
            if gif_:
                do_gifs(envs_gif, agent, model_dict, update_current_state, update_rewards, total_num_steps)
            # #make vae prob gif
            if vae_:
                # do_prob_state(envs_vae, agent, model_dict, vae, update_current_state, total_num_steps)
                # do_gifs2(envs_vae, agent_exploit, vae, model_dict, update_current_state, update_rewards, total_num_steps)
                do_gifs3(envs_vae, agent_exploit, vae, model_dict, update_current_state, update_rewards, total_num_steps)



        #Print updates
        if j % log_interval == 0:# and j!=0:
            end = time.time()

            

            to_print_info_string = "{}, {}, {:.1f}/{:.1f}/{:.1f}/{:.1f}, {}, {:.1f}, {:.2f}".format(j, total_num_steps,
                                       final_rewards.min(),
                                       final_rewards.median(),
                                       final_rewards.mean(),
                                       final_rewards.max(),
                                       int(total_num_steps / (end - start)),
                                       end - start,
                                       end - start2)

            elbo =  "{:.2f}".format(elbo.data.cpu().numpy()[0])
            # elbo =  "1"

            steps_time = "{:.3f}".format(steps_time)
            update_time = "{:.3f}".format(update_time)

            # if next_state_pred_:
            #     state_pred_error_print =  "{:.2f}".format(agent.state_pred_error.data.cpu().numpy()[0])
            #     print(to_print_info_string+' '+state_pred_error_print+' '+elbo)
            #     to_print_legend_string = "Upts, n_timesteps, min/med/mean/max, FPS, total_T, step_T, pred_error, elbo"

            # else:
            # print(to_print_info_string+' '+elbo)
            # print(to_print_info_string+' '+elbo+' '+str(exploit_processes)+' '+str(before_step_time)+' '+str(step_time)+' '+str(after_step_time))#, value[0].data.cpu().numpy(), m.cpu().numpy(), v.cpu().numpy())
            print(to_print_info_string+' '+elbo+' '+str(exploit_processes))#+' '+steps_time+' '+update_time)#, value[0].data.cpu().numpy(), m.cpu().numpy(), v.cpu().numpy())

            # print (value[0].data.cpu().numpy(), m.cpu().numpy(), v.cpu().numpy())
            to_print_legend_string = "Upts, n_timesteps, min/med/mean/max, FPS, total_T, step_T, elbo, Exploit_Procs"

            
            start2 = time.time()


            
            if j % (log_interval*30) == 0:
            
                if ls_:
                    # do_ls(envs_ls, agent, model_dict, total_num_steps, update_current_state, update_rewards)
                    do_ls_2(envs_ls, agent_explore, model_dict, total_num_steps, update_current_state, update_rewards)

                # print("Upts, n_timesteps, min/med/mean/max, FPS, Time, Plot updated, LS updated")
                # print(to_print_info_string + ' LS recorded')#, agent.current_lr)
                # else:
                #update plots
                try:
                    if ls_:
                        # update_ls_plot(model_dict)
                        update_ls_plot_2(model_dict)



        
                    start3 = time.time()


                    make_plots(model_dict)
                    print(to_print_legend_string + " Plot updated ")#+str(time.time() - start3))
                except:
                    raise #pass
                    print(to_print_legend_string)



    try:
        make_plots(model_dict)
    except:
        print ()
        # pass #raise





if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--m')
    args = parser.parse_args()

    #Load model dict 
    with open(args.m, 'r') as infile:
        model_dict = json.load(infile)


    train(model_dict)




